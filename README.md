# Image Captioning Fine-Tuning

This project fine-tunes a **vision-language transformer** model (`microsoft/git-base`) for **image captioning** using the **One Piece BLIP Captions** dataset. The model learns to generate descriptive captions for anime-style images, followed by evaluation and inference steps.

---

## Features

- Fine-tunes `microsoft/git-base` model on custom anime caption dataset.
- Filters out unwanted samples containing specific keywords.
- Evaluates model performance using **Word Error Rate (WER)**.
- Generates captions for new input images.
- Utilizes the Hugging Face `transformers`, `datasets`, and `evaluate` libraries.

---

## Hello and Welcome

Thank you for visiting my GitHub page!  
I’m currently in the process of uploading my previous projects and doing my best to make them publicly available in an organized and clear manner.  

At the moment, I’m adding each project with some **light updates and clarifying adjustments**, and I plan to include more **comprehensive notes, explanations, and detailed READMEs** very soon — to help others better understand each project’s idea and usage.  

If you have any questions, suggestions, or would simply like to connect, I’d be delighted to hear from you:  
- [LinkedIn](https://www.linkedin.com/in/wajdalrabiah)  
- [X (Twitter)](https://x.com/wajdalrabiah)  

Thank you again for your time and interest.  
Your feedback and interaction are always appreciated

> _More updates coming soon — stay tuned!_  
